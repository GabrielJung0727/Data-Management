{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ancestor9/2025_Spring_Data-Management/blob/main/week_06/Text_Representation_and_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUYhh4devHeN"
      },
      "source": [
        "# 텍스트 표현 기법과 임베딩\n",
        "# **Data Representation**\n",
        "\n",
        "## 1. Tabular data\n",
        "<img src='http://jalammar.github.io/images/pandas-intro/0%20excel-to-pandas.png'>\n",
        "\n",
        "## 2. Audio and Timeseries data\n",
        "<img src= 'http://jalammar.github.io/images/numpy/numpy-audio.png'>\n",
        "\n",
        "## 3. Image data\n",
        "<img src='http://jalammar.github.io/images/numpy/numpy-grayscale-image.png'>\n",
        "<img src='http://jalammar.github.io/images/numpy/numpy-color-image.png'>\n",
        "\n",
        "## <font color='orange'>**4. Text data**\n",
        "- **아래 그림을 이해하여야 한다.**\n",
        "<img src='http://jalammar.github.io/images/numpy/numpy-nlp-embeddings.png'>\n",
        "<img src='http://jalammar.github.io/images/numpy/numpy-nlp-bert-shape.png'>\n",
        "\n",
        "\n",
        "## <font color='orange'>**목소리, 주식가격, 그림, 동영상, 언어는 모두 순서(Order)가 있다.**"
      ],
      "id": "cUYhh4devHeN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33pJFFSjvHeQ"
      },
      "source": [
        "## 🎯 강의 목표\n",
        "- 자연어 처리에서 사용되는 텍스트 표현 방법의 역사와 원리를 이해한다.\n",
        "- Bag of Words, TF-IDF, Word Embedding 기법을 실습한다.\n",
        "- Word2Vec과 같은 사전 학습된 임베딩을 적용해 본다."
      ],
      "id": "33pJFFSjvHeQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSCZdepMvHeR"
      },
      "source": [
        "## 📘 이론 강의\n",
        "### **1. 텍스트 표현(Representation of Text)의 필요성**\n",
        "- 컴퓨터는 텍스트를 숫자로 이해해야 함\n",
        "- 자연어는 비정형 데이터 → 수치화 필요\n",
        "\n",
        "### **2. 텍스트 표현 방식**\n",
        "#### 2.1 One-hot Encoding\n",
        "- 각 단어를 고유 인덱스로 변환 후, 그 인덱스만 1인 벡터로 표현\n",
        "- 단점: 희소성, 단어 간 의미 관계 없음\n",
        "\n",
        "#### 2.2 Bag of Words (BoW)\n",
        "- 문서별 단어 출현 빈도 벡터\n",
        "- 장점: 단순하고 빠름\n",
        "- 단점: 문맥 정보 손실\n",
        "- 📌 구현 도구: `CountVectorizer`\n",
        "\n",
        "#### 2.3 N-gram 모델\n",
        "- 연속된 N개의 단어를 하나의 특징으로 간주\n",
        "- 예: bigram(\"I love NLP\") → [\"I love\", \"love NLP\"]\n",
        "- 📌 구현 도구: `CountVectorizer(ngram_range=(n, n))`\n",
        "\n",
        "#### 2.4 TF-IDF\n",
        "- 단어의 중요도를 반영한 벡터\n",
        "- TF: 문서 내 빈도 / IDF: 전체 문서에서의 희귀성\n",
        "- 📌 구현 도구: `TfidfVectorizer`\n",
        "\n",
        "#### 2.5 Word Embedding\n",
        "- 의미 기반 분산 표현 (Distributed Representation)\n",
        "- Word2Vec, GloVe, FastText 등\n",
        "- 단어 간 유사도, 의미 추론 가능\n",
        "- CBOW / Skip-gram\n",
        "\n",
        "#### 2.6 사전학습 임베딩\n",
        "- Gensim / HuggingFace Transformers 사용 가능\n",
        "- 예: word2vec-google-news-300"
      ],
      "id": "lSCZdepMvHeR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxdzGGhUvHeS"
      },
      "source": [
        "## 💻 **실습**\n",
        "\n",
        "#### 2.0 Integer Encoding\n",
        "- 📌 구현 도구: `Tokenizer`\n"
      ],
      "id": "RxdzGGhUvHeS"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "61FcFWOSxr5r"
      },
      "id": "61FcFWOSxr5r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 2.1 One-hot Encoding\n",
        "- 📌 구현 도구: `CountVectorizer`\n",
        "\n",
        "#### 2.2 Bag of Words (BoW)\n",
        "- 📌 구현 도구: `CountVectorizer`\n",
        "\n",
        "#### 2.3 N-gram 모델\n",
        "- 📌 구현 도구: `CountVectorizer(ngram_range=(n, n))`\n",
        "\n",
        "#### 2.4 TF-IDF\n",
        "- 📌 구현 도구: `TfidfVectorizer`\n",
        "\n",
        "#### 2.5 Word Embedding\n",
        "- 📌 구현 도구: 'Word2Vec, GloVe, FastText, CBOW / Skip-gram'\n",
        "\n",
        "#### 2.6 사전학습 임베딩\n",
        "- 📌 구현 도구: Gensim / HuggingFace Transformers(word2vec-google-news-300)"
      ],
      "metadata": {
        "id": "Ki75EqrkxogK"
      },
      "id": "Ki75EqrkxogK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKO4QgTuvHeS"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "docs = [\n",
        "    \"I love natural language processing\",\n",
        "    \"Language models are amazing\",\n",
        "    \"I enjoy learning about embeddings\",\n",
        "    \"Embeddings capture semantic meaning\",\n",
        "    \"Natural language is complex\"\n",
        "]\n",
        "vectorizer = CountVectorizer()\n",
        "bow = vectorizer.fit_transform(docs)\n",
        "print(\"BoW Feature Names:\", vectorizer.get_feature_names_out())\n",
        "print(\"BoW Matrix:\\n\", bow.toarray())"
      ],
      "id": "DKO4QgTuvHeS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-Q8LjfCvHeT"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf.fit_transform(docs)\n",
        "print(\"TF-IDF Feature Names:\", tfidf.get_feature_names_out())\n",
        "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())"
      ],
      "id": "I-Q8LjfCvHeT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEmcqoiuvHeT"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "vectorizer_bigram = CountVectorizer(ngram_range=(2,2))\n",
        "bow_bigram = vectorizer_bigram.fit_transform(docs)\n",
        "print(\"Bi-gram Feature Names:\", vectorizer_bigram.get_feature_names_out())\n",
        "print(\"Bi-gram BoW Matrix:\\n\", bow_bigram.toarray())"
      ],
      "id": "XEmcqoiuvHeT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpIVnr9xvHeU"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "tokenized_docs = [doc.lower().split() for doc in docs]\n",
        "w2v_model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=3, min_count=1, sg=1)\n",
        "print(\"Vector for 'language':\", w2v_model.wv['language'])\n",
        "print(\"Similarity between 'language' and 'natural':\", w2v_model.wv.similarity('language', 'natural'))\n",
        "print(\"Most similar to 'embeddings':\", w2v_model.wv.most_similar('embeddings', topn=3))"
      ],
      "id": "IpIVnr9xvHeU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqlWKH_HvHeU"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "model = api.load(\"word2vec-google-news-300\")\n",
        "print(model['computer'])\n",
        "print(model.most_similar(\"language\"))\n",
        "print(\"Similarity between 'language' and 'computer':\", model.similarity('language', 'computer'))"
      ],
      "id": "bqlWKH_HvHeU"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}