{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ancestor9/2025_Spring_Data-Management/blob/main/week_05/Text_Representation_and_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUYhh4devHeN"
      },
      "source": [
        "# 텍스트 표현 기법과 임베딩\n",
        "# **Data Representation**\n",
        "\n",
        "## 1. Tabular data\n",
        "<img src='http://jalammar.github.io/images/pandas-intro/0%20excel-to-pandas.png'>\n",
        "\n",
        "## 2. Audio and Timeseries data\n",
        "<img src= 'http://jalammar.github.io/images/numpy/numpy-audio.png'>\n",
        "\n",
        "## 3. Image data\n",
        "<img src='http://jalammar.github.io/images/numpy/numpy-grayscale-image.png'>\n",
        "<img src='http://jalammar.github.io/images/numpy/numpy-color-image.png'>\n",
        "\n",
        "## <font color='orange'>**4. Text data**\n",
        "- **아래 그림을 이해하여야 한다.**\n",
        "<img src='http://jalammar.github.io/images/numpy/numpy-nlp-embeddings.png'>\n",
        "<img src='http://jalammar.github.io/images/numpy/numpy-nlp-bert-shape.png'>\n",
        "\n",
        "\n",
        "## <font color='orange'>**목소리, 주식가격, 그림, 동영상, 언어는 모두 순서(Order)가 있다.**"
      ],
      "id": "cUYhh4devHeN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33pJFFSjvHeQ"
      },
      "source": [
        "## 🎯 강의 목표\n",
        "- 자연어 처리에서 사용되는 텍스트 표현 방법의 역사와 원리를 이해한다.\n",
        "- Bag of Words, TF-IDF, Word Embedding 기법을 실습한다.\n",
        "- Word2Vec과 같은 사전 학습된 임베딩을 적용해 본다."
      ],
      "id": "33pJFFSjvHeQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSCZdepMvHeR"
      },
      "source": [
        "## 📘 이론 강의\n",
        "### **1. 텍스트 표현(Representation of Text)의 필요성**\n",
        "- 컴퓨터는 텍스트를 숫자로 이해해야 함\n",
        "- 자연어는 비정형 데이터 → 수치화 필요\n",
        "\n",
        "### **2. 텍스트 표현 방식**\n",
        "#### 2.1 One-hot Encoding\n",
        "- 각 단어를 고유 인덱스로 변환 후, 그 인덱스만 1인 벡터로 표현\n",
        "- 단점: 희소성, 단어 간 의미 관계 없음\n",
        "\n",
        "#### 2.2 Bag of Words (BoW)\n",
        "- 문서별 단어 출현 빈도 벡터\n",
        "- 장점: 단순하고 빠름\n",
        "- 단점: 문맥 정보 손실\n",
        "- 📌 구현 도구: `CountVectorizer`\n",
        "\n",
        "#### 2.3 N-gram 모델\n",
        "- 연속된 N개의 단어를 하나의 특징으로 간주\n",
        "- 예: bigram(\"I love NLP\") → [\"I love\", \"love NLP\"]\n",
        "- 📌 구현 도구: `CountVectorizer(ngram_range=(n, n))`\n",
        "\n",
        "#### 2.4 TF-IDF\n",
        "- 단어의 중요도를 반영한 벡터\n",
        "- TF: 문서 내 빈도 / IDF: 전체 문서에서의 희귀성\n",
        "- 📌 구현 도구: `TfidfVectorizer`\n",
        "\n",
        "#### 2.5 Word Embedding\n",
        "- 의미 기반 분산 표현 (Distributed Representation)\n",
        "- Word2Vec, GloVe, FastText 등\n",
        "- 단어 간 유사도, 의미 추론 가능\n",
        "- CBOW / Skip-gram\n",
        "\n",
        "#### 2.6 사전학습 임베딩\n",
        "- Gensim / HuggingFace Transformers 사용 가능\n",
        "- 예: word2vec-google-news-300"
      ],
      "id": "lSCZdepMvHeR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxdzGGhUvHeS"
      },
      "source": [
        "## 💻 **실습**\n",
        "\n",
        "#### 2.0 정수인코딩(Integer Encoding)\n",
        "- 📌 구현 도구: `Tokenizer`\n",
        "- 예를 들어 : 단어에 정수를 부여하는 방법 중 하나로 단어를 빈도수 순으로 정렬한 단어 집합(vocabulary)을 만들고, 빈도수가 높은 순서대로 차례로 낮은 숫자부터 정수를 부여하는 방법\n"
      ],
      "id": "RxdzGGhUvHeS"
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = '''\n",
        "A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\n",
        "'''"
      ],
      "metadata": {
        "id": "i3jAB2Uj524_"
      },
      "id": "i3jAB2Uj524_",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab') # 문장을 구분하거나 단어로 쪼갤 때 필요한 pre-trained 모델\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36dUYxkd6QXc",
        "outputId": "636cfdb2-f460-4b7f-fff2-c3b12b2af5d4"
      },
      "id": "36dUYxkd6QXc",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장 토큰화\n",
        "sentences = sent_tokenize(raw_text)\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZvs3umR6YBv",
        "outputId": "8981f2d8-5817-4781-8da3-e537ba928203"
      },
      "id": "mZvs3umR6YBv",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\nA barber is a person.',\n",
              " 'a barber is good person.',\n",
              " 'a barber is huge person.',\n",
              " 'he Knew A Secret!',\n",
              " 'The Secret He Kept is huge secret.',\n",
              " 'Huge secret.',\n",
              " 'His barber kept his word.',\n",
              " 'a barber kept his word.',\n",
              " 'His barber kept his secret.',\n",
              " 'But keeping and keeping such a huge secret to himself was driving the barber crazy.',\n",
              " 'the barber went up a huge mountain.']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVb_2eul68fw",
        "outputId": "8a58d410-7578-4495-ff69-1206d4a059df"
      },
      "id": "OVb_2eul68fw",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in sentences:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e_AYFth6lRG",
        "outputId": "56b8865f-e006-4d5f-9b6b-46002f56c8ad"
      },
      "id": "3e_AYFth6lRG",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "A barber is a person.\n",
            "a barber is good person.\n",
            "a barber is huge person.\n",
            "he Knew A Secret!\n",
            "The Secret He Kept is huge secret.\n",
            "Huge secret.\n",
            "His barber kept his word.\n",
            "a barber kept his word.\n",
            "His barber kept his secret.\n",
            "But keeping and keeping such a huge secret to himself was driving the barber crazy.\n",
            "the barber went up a huge mountain.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0nLGE_o7G7s",
        "outputId": "644e2f8f-9ee0-4679-9fb6-f1a80408ba98"
      },
      "id": "_0nLGE_o7G7s",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vocab = {}\n",
        "preprocessed_sentences = []\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "for sentence in sentences:\n",
        "    # 단어 토큰화\n",
        "    tokenized_sentence = word_tokenize(sentence)\n",
        "    result = []\n",
        "\n",
        "    for word in tokenized_sentence:\n",
        "        word = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄인다.\n",
        "        if word not in stop_words: # 단어 토큰화 된 결과에 대해서 불용어를 제거한다.\n",
        "            if len(word) > 2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거한다.\n",
        "                result.append(word)\n",
        "                if word not in vocab:\n",
        "                    vocab[word] = 0\n",
        "                vocab[word] += 1\n",
        "    preprocessed_sentences.append(result)\n",
        "print(preprocessed_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZHeCqyE8qbF",
        "outputId": "6dabc246-c5b2-4a31-e494-e65ac2316554"
      },
      "id": "LZHeCqyE8qbF",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('단어 집합 :',vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZmssNVS7lPu",
        "outputId": "ea6e31cf-b951-4016-91e3-2231cc42622c"
      },
      "id": "CZmssNVS7lPu",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합 : {'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)\n",
        "vocab_sorted"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aMP3RFW7qG3",
        "outputId": "4e3098eb-2391-4a60-a9b3-7bf613be6f4a"
      },
      "id": "1aMP3RFW7qG3",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('barber', 8),\n",
              " ('secret', 6),\n",
              " ('huge', 5),\n",
              " ('kept', 4),\n",
              " ('person', 3),\n",
              " ('word', 2),\n",
              " ('keeping', 2),\n",
              " ('good', 1),\n",
              " ('knew', 1),\n",
              " ('driving', 1),\n",
              " ('crazy', 1),\n",
              " ('went', 1),\n",
              " ('mountain', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 높은 빈도수를 가진 단어일수록 낮은 정수를 1부터 부여\n",
        "word_to_index = {}\n",
        "i = 0\n",
        "for (word, frequency) in vocab_sorted :\n",
        "    if frequency > 1 : # 빈도수가 작은 단어는 제외.\n",
        "        i = i + 1\n",
        "        word_to_index[word] = i\n",
        "\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-Plou9y86iV",
        "outputId": "41ae46e1-1fc9-4e9d-f56b-d953f2498bf3"
      },
      "id": "h-Plou9y86iV",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 자연어 처리를 하다보면, 텍스트 데이터에 있는 단어를 모두 사용하기 보다는 빈도수가 가장 높은 n개의 단어만 사용하고 싶을 때\n",
        "vocab_size = 5\n",
        "\n",
        "# 인덱스가 5 초과인 단어 제거\n",
        "words_frequency = [word for word, index in word_to_index.items() if index >= vocab_size + 1]\n",
        "\n",
        "# 해당 단어에 대한 인덱스 정보를 삭제\n",
        "for w in words_frequency:\n",
        "    del word_to_index[w]\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Khblo_Xp9OQw",
        "outputId": "a54cdf36-5c6e-46d1-8c2d-1924be3b5238"
      },
      "id": "Khblo_Xp9OQw",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 집합에 존재하지 않는 단어들이 생기는 상황을 Out-Of-Vocabulary(단어 집합에 없는 단어) 'OOV 문제'\n",
        "# word_to_index에 'OOV'란 단어를 새롭게 추가하고, 단어 집합에 없는 단어들은 'OOV'의 인덱스로 인코딩"
      ],
      "metadata": {
        "id": "SOGqyKk--EXu"
      },
      "id": "SOGqyKk--EXu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index['OOV'] = len(word_to_index) + 1\n",
        "print(word_to_index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tT0Ad9OM-EbD",
        "outputId": "56315130-2582-4170-8329-0e9e3c1ec01c"
      },
      "id": "tT0Ad9OM-EbD",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'OOV': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_sentences = []\n",
        "for sentence in preprocessed_sentences:\n",
        "    encoded_sentence = []\n",
        "    for word in sentence:\n",
        "        try:\n",
        "            # 단어 집합에 있는 단어라면 해당 단어의 정수를 리턴.\n",
        "            encoded_sentence.append(word_to_index[word])\n",
        "        except KeyError:\n",
        "            # 만약 단어 집합에 없는 단어라면 'OOV'의 정수를 리턴.\n",
        "            encoded_sentence.append(word_to_index['OOV'])\n",
        "    encoded_sentences.append(encoded_sentence)\n",
        "\n",
        "print(encoded_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PBLu0Zb-Zrq",
        "outputId": "e6531030-de1a-4097-8053-0ccc34a9b543"
      },
      "id": "0PBLu0Zb-Zrq",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Counter**"
      ],
      "metadata": {
        "id": "AMB44tjg-kKM"
      },
      "id": "AMB44tjg-kKM"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "CdC-Fa6X-jP4"
      },
      "id": "CdC-Fa6X-jP4",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSPBzPPV-qb3",
        "outputId": "78cf91a9-6878-41a1-c9bd-7f12a8ae471c"
      },
      "id": "jSPBzPPV-qb3",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['barber', 'person'],\n",
              " ['barber', 'good', 'person'],\n",
              " ['barber', 'huge', 'person'],\n",
              " ['knew', 'secret'],\n",
              " ['secret', 'kept', 'huge', 'secret'],\n",
              " ['huge', 'secret'],\n",
              " ['barber', 'kept', 'word'],\n",
              " ['barber', 'kept', 'word'],\n",
              " ['barber', 'kept', 'secret'],\n",
              " ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
              " ['barber', 'went', 'huge', 'mountain']]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: preprocessed_sentences를 하나의 단어집합으로 만들어줘\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "# Flatten the list of lists into a single list of words\n",
        "all_words = [word for sentence in preprocessed_sentences for word in sentence]\n",
        "\n",
        "# Create a vocabulary using Counter\n",
        "vocab = Counter(all_words)\n",
        "\n",
        "vocab\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_6UcS_V-wrM",
        "outputId": "9412ca30-3951-4626-dd09-1bf96fe7951d"
      },
      "id": "i_6UcS_V-wrM",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'barber': 8,\n",
              "         'person': 3,\n",
              "         'good': 1,\n",
              "         'huge': 5,\n",
              "         'knew': 1,\n",
              "         'secret': 6,\n",
              "         'kept': 4,\n",
              "         'word': 2,\n",
              "         'keeping': 2,\n",
              "         'driving': 1,\n",
              "         'crazy': 1,\n",
              "         'went': 1,\n",
              "         'mountain': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# words = np.hstack(preprocessed_sentences)으로도 수행 가능.\n",
        "all_words = sum(preprocessed_sentences, [])\n",
        "Counter(all_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dhb6Dvug-qd_",
        "outputId": "0a957140-56fe-41d5-f121-8962e217b12c"
      },
      "id": "Dhb6Dvug-qd_",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'barber': 8,\n",
              "         'person': 3,\n",
              "         'good': 1,\n",
              "         'huge': 5,\n",
              "         'knew': 1,\n",
              "         'secret': 6,\n",
              "         'kept': 4,\n",
              "         'word': 2,\n",
              "         'keeping': 2,\n",
              "         'driving': 1,\n",
              "         'crazy': 1,\n",
              "         'went': 1,\n",
              "         'mountain': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab[\"barber\"]) # 'barber'라는 단어의 빈도수 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYWwI5Zf-qf9",
        "outputId": "5f87e03a-0458-4375-caef-fdd87a495e8f"
      },
      "id": "OYWwI5Zf-qf9",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 5\n",
        "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzrdmZsx-qh_",
        "outputId": "48781cd9-349a-4ebd-df41-ef437f7f373b"
      },
      "id": "hzrdmZsx-qh_",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='red'> **퀴즈 1. 이상한 나라의 앨리스라는 소설에서 출현하는 상위 10위 단어는 무엇인가?**"
      ],
      "metadata": {
        "id": "1egoGVtjBcQE"
      },
      "id": "1egoGVtjBcQE"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import gutenberg\n",
        "# 필요한 리소스를 다운로드\n",
        "nltk.download('gutenberg')\n",
        "# 구텐베르크 말뭉치에서 사용 가능한 파일 목록 출력\n",
        "file_ids = gutenberg.fileids()\n",
        "file_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLrNAg0E1ppo",
        "outputId": "392ddec1-dd5d-43ad-c8dc-40bf25dded49"
      },
      "id": "HLrNAg0E1ppo",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = gutenberg.raw('austen-emma.txt')\n",
        "raw_text[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "hTuYZRTt4ypE",
        "outputId": "900ddad7-aac6-4b7f-f057-9c59077da75b"
      },
      "id": "hTuYZRTt4ypE",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAPTER I\\n\\n\\nEmma Woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to unite some of the best blessings\\nof existence; and had lived nearly twenty-one years in the world\\nwith very little to distress or vex her.\\n\\nShe was the youngest of the two daughters of a most affectionate,\\nindulgent father; and had, in consequence of her sister's marriage,\\nbeen mistress of his house from a very early period.  Her mother\\nhad died too long ago for her to have more than an indistinct\\nremembrance of her caresses; and her place had been supplied\\nby an excellent woman as governess, who had fallen little short\\nof a mother in affection.\\n\\nSixteen years had Miss Taylor been in Mr. Woodhouse's family,\\nless as a governess than a friend, very fond of both daughters,\\nbut particularly of Emma.  Between _them_ it was more the intimacy\\nof sisters.  Even before Miss Taylor had ceased to hold the nominal\\noffice of governess, the mildness o\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: raw_text에서 출현하는 단어 사우이 10개와 출현빈도를 구해줘\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(raw_text)\n",
        "\n",
        "# Remove stop words and punctuation\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = [w.lower() for w in tokens if w.isalnum() and w.lower() not in stop_words]\n",
        "\n",
        "# Count word frequencies\n",
        "word_counts = Counter(words)\n",
        "\n",
        "# Get the top 10 most frequent words\n",
        "top_10_words = word_counts.most_common(10)\n",
        "\n",
        "top_10_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehNwTHnz4yyc",
        "outputId": "8ca3abb9-6993-42dc-aebb-ac24bfc588b7"
      },
      "id": "ehNwTHnz4yyc",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('emma', 860),\n",
              " ('could', 836),\n",
              " ('would', 818),\n",
              " ('miss', 599),\n",
              " ('must', 566),\n",
              " ('harriet', 500),\n",
              " ('much', 484),\n",
              " ('said', 483),\n",
              " ('one', 447),\n",
              " ('weston', 437)]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 2.1 One-hot Encoding\n",
        "- 📌 구현 도구: `CountVectorizer`\n",
        "\n",
        "#### 2.2 Bag of Words (BoW)\n",
        "- 📌 구현 도구: `CountVectorizer`\n",
        "\n",
        "#### 2.3 N-gram 모델\n",
        "- 📌 구현 도구: `CountVectorizer(ngram_range=(n, n))`\n",
        "\n",
        "#### 2.4 TF-IDF\n",
        "- 📌 구현 도구: `TfidfVectorizer`\n",
        "\n",
        "#### 2.5 Word Embedding\n",
        "- 📌 구현 도구: 'Word2Vec, GloVe, FastText, CBOW / Skip-gram'\n",
        "\n",
        "#### 2.6 사전학습 임베딩\n",
        "- 📌 구현 도구: Gensim / HuggingFace Transformers(word2vec-google-news-300)"
      ],
      "metadata": {
        "id": "Ki75EqrkxogK"
      },
      "id": "Ki75EqrkxogK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKO4QgTuvHeS"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "docs = [\n",
        "    \"I love natural language processing\",\n",
        "    \"Language models are amazing\",\n",
        "    \"I enjoy learning about embeddings\",\n",
        "    \"Embeddings capture semantic meaning\",\n",
        "    \"Natural language is complex\"\n",
        "]\n",
        "vectorizer = CountVectorizer()\n",
        "bow = vectorizer.fit_transform(docs)\n",
        "print(\"BoW Feature Names:\", vectorizer.get_feature_names_out())\n",
        "print(\"BoW Matrix:\\n\", bow.toarray())"
      ],
      "id": "DKO4QgTuvHeS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-Q8LjfCvHeT"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf.fit_transform(docs)\n",
        "print(\"TF-IDF Feature Names:\", tfidf.get_feature_names_out())\n",
        "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())"
      ],
      "id": "I-Q8LjfCvHeT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEmcqoiuvHeT"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "vectorizer_bigram = CountVectorizer(ngram_range=(2,2))\n",
        "bow_bigram = vectorizer_bigram.fit_transform(docs)\n",
        "print(\"Bi-gram Feature Names:\", vectorizer_bigram.get_feature_names_out())\n",
        "print(\"Bi-gram BoW Matrix:\\n\", bow_bigram.toarray())"
      ],
      "id": "XEmcqoiuvHeT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpIVnr9xvHeU"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "tokenized_docs = [doc.lower().split() for doc in docs]\n",
        "w2v_model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=3, min_count=1, sg=1)\n",
        "print(\"Vector for 'language':\", w2v_model.wv['language'])\n",
        "print(\"Similarity between 'language' and 'natural':\", w2v_model.wv.similarity('language', 'natural'))\n",
        "print(\"Most similar to 'embeddings':\", w2v_model.wv.most_similar('embeddings', topn=3))"
      ],
      "id": "IpIVnr9xvHeU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqlWKH_HvHeU"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "model = api.load(\"word2vec-google-news-300\")\n",
        "print(model['computer'])\n",
        "print(model.most_similar(\"language\"))\n",
        "print(\"Similarity between 'language' and 'computer':\", model.similarity('language', 'computer'))"
      ],
      "id": "bqlWKH_HvHeU"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}