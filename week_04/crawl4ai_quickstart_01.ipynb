{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## [crawl4ai](https://docs.crawl4ai.com/core/installation/)"
      ],
      "metadata": {
        "id": "0JTzoBamcLi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install crawl4ai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsZMF1-QckZQ",
        "outputId": "21b1ecd2-e313-489f-a484-14f9d1e27637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: crawl4ai in /usr/local/lib/python3.11/dist-packages (0.5.0.post4)\n",
            "Requirement already satisfied: aiosqlite~=0.20 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (0.21.0)\n",
            "Requirement already satisfied: lxml~=5.3 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (5.3.1)\n",
            "Requirement already satisfied: litellm>=1.53.1 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (1.63.12)\n",
            "Requirement already satisfied: numpy<3,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (2.0.2)\n",
            "Requirement already satisfied: pillow~=10.4 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (10.4.0)\n",
            "Requirement already satisfied: playwright>=1.49.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (1.51.0)\n",
            "Requirement already satisfied: python-dotenv~=1.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (1.0.1)\n",
            "Requirement already satisfied: requests~=2.26 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4~=4.12 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (4.13.3)\n",
            "Requirement already satisfied: tf-playwright-stealth>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (1.1.2)\n",
            "Requirement already satisfied: xxhash~=3.4 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (3.5.0)\n",
            "Requirement already satisfied: rank-bm25~=0.2 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (0.2.2)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (24.1.0)\n",
            "Requirement already satisfied: colorama~=0.4 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (0.4.6)\n",
            "Requirement already satisfied: snowballstemmer~=2.2 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (2.2.0)\n",
            "Requirement already satisfied: pydantic>=2.10 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (2.10.6)\n",
            "Requirement already satisfied: pyOpenSSL>=24.3.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (25.0.0)\n",
            "Requirement already satisfied: psutil>=6.1.1 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (7.0.0)\n",
            "Requirement already satisfied: nltk>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (3.9.1)\n",
            "Requirement already satisfied: rich>=13.9.4 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (13.9.4)\n",
            "Requirement already satisfied: cssselect>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.27.2 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (0.28.1)\n",
            "Requirement already satisfied: fake-useragent>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (2.1.0)\n",
            "Requirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (8.1.8)\n",
            "Requirement already satisfied: pyperclip>=1.8.2 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (1.9.0)\n",
            "Requirement already satisfied: faust-cchardet>=2.1.19 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (2.1.19)\n",
            "Requirement already satisfied: aiohttp>=3.11.11 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (3.11.14)\n",
            "Requirement already satisfied: humanize>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (4.12.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.11.11->crawl4ai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.11.11->crawl4ai) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.11.11->crawl4ai) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.11.11->crawl4ai) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.11.11->crawl4ai) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.11.11->crawl4ai) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.11.11->crawl4ai) (1.18.3)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.11/dist-packages (from aiosqlite~=0.20->crawl4ai) (4.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4~=4.12->crawl4ai) (2.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->crawl4ai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->crawl4ai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->crawl4ai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->crawl4ai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.2->crawl4ai) (0.14.0)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (8.6.1)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (4.23.0)\n",
            "Requirement already satisfied: openai>=1.66.1 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (1.66.3)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (0.9.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (0.21.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9.1->crawl4ai) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9.1->crawl4ai) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9.1->crawl4ai) (4.67.1)\n",
            "Requirement already satisfied: pyee<13,>=12 in /usr/local/lib/python3.11/dist-packages (from playwright>=1.49.0->crawl4ai) (12.1.1)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright>=1.49.0->crawl4ai) (3.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->crawl4ai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->crawl4ai) (2.27.2)\n",
            "Requirement already satisfied: cryptography<45,>=41.0.5 in /usr/local/lib/python3.11/dist-packages (from pyOpenSSL>=24.3.0->crawl4ai) (43.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.26->crawl4ai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.26->crawl4ai) (2.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.9.4->crawl4ai) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.9.4->crawl4ai) (2.18.0)\n",
            "Requirement already satisfied: fake-http-header<0.4.0,>=0.3.5 in /usr/local/lib/python3.11/dist-packages (from tf-playwright-stealth>=1.1.0->crawl4ai) (0.3.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<45,>=41.0.5->pyOpenSSL>=24.3.0->crawl4ai) (1.17.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.8.0->litellm>=1.53.1->crawl4ai) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.53.1->crawl4ai) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (0.23.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->crawl4ai) (0.1.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.66.1->litellm>=1.53.1->crawl4ai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.66.1->litellm>=1.53.1->crawl4ai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.66.1->litellm>=1.53.1->crawl4ai) (1.3.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers->litellm>=1.53.1->crawl4ai) (0.29.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<45,>=41.0.5->pyOpenSSL>=24.3.0->crawl4ai) (2.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! crawl4ai-setup\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo5ZxUsncpBq",
        "outputId": "fa2d4a3a-72ad-40e9-c791-cb2f2cb44f1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m[INIT].... → Running post-installation setup...\u001b[0m\n",
            "\u001b[36m[INIT].... → Installing Playwright browsers...\u001b[0m\n",
            "Installing dependencies...\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,003 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,962 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,535 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,677 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,762 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,237 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,692 kB]\n",
            "Fetched 24.3 MB in 3s (8,935 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "fonts-liberation is already the newest version (1:1.07.4-11).\n",
            "libasound2 is already the newest version (1.2.6.1-1ubuntu1).\n",
            "libasound2 set to manually installed.\n",
            "libatk-bridge2.0-0 is already the newest version (2.38.0-3).\n",
            "libatk-bridge2.0-0 set to manually installed.\n",
            "libatk1.0-0 is already the newest version (2.36.0-3build1).\n",
            "libatk1.0-0 set to manually installed.\n",
            "libatspi2.0-0 is already the newest version (2.44.0-3).\n",
            "libatspi2.0-0 set to manually installed.\n",
            "libcairo2 is already the newest version (1.16.0-5ubuntu2).\n",
            "libcairo2 set to manually installed.\n",
            "libfontconfig1 is already the newest version (2.13.1-4.2ubuntu5).\n",
            "libfontconfig1 set to manually installed.\n",
            "libxcb1 is already the newest version (1.14-3ubuntu3).\n",
            "libxcb1 set to manually installed.\n",
            "libxcomposite1 is already the newest version (1:0.4.5-1build2).\n",
            "libxcomposite1 set to manually installed.\n",
            "libxdamage1 is already the newest version (1:1.1.5-2build2).\n",
            "libxdamage1 set to manually installed.\n",
            "libxext6 is already the newest version (2:1.3.4-1build1).\n",
            "libxfixes3 is already the newest version (1:6.0.0-1).\n",
            "libxfixes3 set to manually installed.\n",
            "libxkbcommon0 is already the newest version (1.4.0-1).\n",
            "libxkbcommon0 set to manually installed.\n",
            "libxrandr2 is already the newest version (2:1.5.2-1build1).\n",
            "libxrandr2 set to manually installed.\n",
            "libcups2 is already the newest version (2.4.1op1-1ubuntu4.11).\n",
            "libcups2 set to manually installed.\n",
            "libdbus-1-3 is already the newest version (1.12.20-2ubuntu4.1).\n",
            "libdbus-1-3 set to manually installed.\n",
            "libdrm2 is already the newest version (2.4.113-2~ubuntu0.22.04.1).\n",
            "libdrm2 set to manually installed.\n",
            "libfreetype6 is already the newest version (2.11.1+dfsg-1ubuntu0.3).\n",
            "libfreetype6 set to manually installed.\n",
            "libgbm1 is already the newest version (23.2.1-1ubuntu3.1~22.04.3).\n",
            "libgbm1 set to manually installed.\n",
            "libglib2.0-0 is already the newest version (2.72.4-0ubuntu2.4).\n",
            "libglib2.0-0 set to manually installed.\n",
            "libnspr4 is already the newest version (2:4.35-0ubuntu0.22.04.1).\n",
            "libnspr4 set to manually installed.\n",
            "libnss3 is already the newest version (2:3.98-0ubuntu0.22.04.2).\n",
            "libnss3 set to manually installed.\n",
            "libpango-1.0-0 is already the newest version (1.50.6+ds-2ubuntu1).\n",
            "libpango-1.0-0 set to manually installed.\n",
            "libwayland-client0 is already the newest version (1.20.0-1ubuntu0.1).\n",
            "libwayland-client0 set to manually installed.\n",
            "libx11-6 is already the newest version (2:1.7.5-1ubuntu0.3).\n",
            "libx11-6 set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-encodings xfonts-utils xserver-common\n",
            "Recommended packages:\n",
            "  fonts-ipafont-mincho fonts-tlwg-loma xfonts-base\n",
            "The following NEW packages will be installed:\n",
            "  fonts-freefont-ttf fonts-ipafont-gothic fonts-noto-color-emoji fonts-tlwg-loma-otf fonts-unifont\n",
            "  fonts-wqy-zenhei libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-cyrillic xfonts-encodings\n",
            "  xfonts-scalable xfonts-utils xserver-common xvfb\n",
            "0 upgraded, 16 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 29.7 MB of archives.\n",
            "After this operation, 71.4 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-ipafont-gothic all 00303-21ubuntu1 [3,513 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-freefont-ttf all 20120503-10build1 [2,388 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 fonts-noto-color-emoji all 2.047-0ubuntu0.22.04.1 [10.0 MB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-tlwg-loma-otf all 1:0.7.3-1 [107 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-unifont all 1:14.0.01-1 [3,551 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-wqy-zenhei all 0.9.45-8 [7,472 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 xfonts-cyrillic all 1:1.0.5 [386 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-scalable all 1:1.0.3-1.2ubuntu1 [306 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.13 [29.1 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.13 [863 kB]\n",
            "Fetched 29.7 MB in 0s (59.6 MB/s)\n",
            "Selecting previously unselected package fonts-ipafont-gothic.\n",
            "(Reading database ... 126209 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-ipafont-gothic_00303-21ubuntu1_all.deb ...\n",
            "Unpacking fonts-ipafont-gothic (00303-21ubuntu1) ...\n",
            "Selecting previously unselected package fonts-freefont-ttf.\n",
            "Preparing to unpack .../01-fonts-freefont-ttf_20120503-10build1_all.deb ...\n",
            "Unpacking fonts-freefont-ttf (20120503-10build1) ...\n",
            "Selecting previously unselected package fonts-noto-color-emoji.\n",
            "Preparing to unpack .../02-fonts-noto-color-emoji_2.047-0ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking fonts-noto-color-emoji (2.047-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package fonts-tlwg-loma-otf.\n",
            "Preparing to unpack .../03-fonts-tlwg-loma-otf_1%3a0.7.3-1_all.deb ...\n",
            "Unpacking fonts-tlwg-loma-otf (1:0.7.3-1) ...\n",
            "Selecting previously unselected package fonts-unifont.\n",
            "Preparing to unpack .../04-fonts-unifont_1%3a14.0.01-1_all.deb ...\n",
            "Unpacking fonts-unifont (1:14.0.01-1) ...\n",
            "Selecting previously unselected package fonts-wqy-zenhei.\n",
            "Preparing to unpack .../05-fonts-wqy-zenhei_0.9.45-8_all.deb ...\n",
            "Unpacking fonts-wqy-zenhei (0.9.45-8) ...\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "Preparing to unpack .../06-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../07-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../08-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../09-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../10-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../11-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-cyrillic.\n",
            "Preparing to unpack .../12-xfonts-cyrillic_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-cyrillic (1:1.0.5) ...\n",
            "Selecting previously unselected package xfonts-scalable.\n",
            "Preparing to unpack .../13-xfonts-scalable_1%3a1.0.3-1.2ubuntu1_all.deb ...\n",
            "Unpacking xfonts-scalable (1:1.0.3-1.2ubuntu1) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../14-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.13_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.13) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../15-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.13_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.13) ...\n",
            "Setting up fonts-noto-color-emoji (2.047-0ubuntu0.22.04.1) ...\n",
            "Setting up fonts-wqy-zenhei (0.9.45-8) ...\n",
            "Setting up fonts-freefont-ttf (20120503-10build1) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up fonts-tlwg-loma-otf (1:0.7.3-1) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up fonts-ipafont-gothic (00303-21ubuntu1) ...\n",
            "update-alternatives: using /usr/share/fonts/opentype/ipafont-gothic/ipag.ttf to provide /usr/share/fonts/truetype/fonts-japanese-gothic.ttf (fonts-japanese-gothic.ttf) in auto mode\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up fonts-unifont (1:14.0.01-1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-cyrillic (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.13) ...\n",
            "Setting up xfonts-scalable (1:1.0.3-1.2ubuntu1) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.13) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "Downloading Chromium 134.0.6998.35 (playwright build v1161)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1161/chromium-linux.zip\u001b[22m\n",
            "\u001b[1G164.9 MiB [] 0% 0.0s\u001b[0K\u001b[1G164.9 MiB [] 0% 28.6s\u001b[0K\u001b[1G164.9 MiB [] 0% 11.9s\u001b[0K\u001b[1G164.9 MiB [] 0% 6.4s\u001b[0K\u001b[1G164.9 MiB [] 1% 4.3s\u001b[0K\u001b[1G164.9 MiB [] 2% 4.0s\u001b[0K\u001b[1G164.9 MiB [] 2% 3.8s\u001b[0K\u001b[1G164.9 MiB [] 3% 3.5s\u001b[0K\u001b[1G164.9 MiB [] 4% 2.9s\u001b[0K\u001b[1G164.9 MiB [] 5% 2.6s\u001b[0K\u001b[1G164.9 MiB [] 6% 2.5s\u001b[0K\u001b[1G164.9 MiB [] 6% 2.6s\u001b[0K\u001b[1G164.9 MiB [] 7% 2.6s\u001b[0K\u001b[1G164.9 MiB [] 8% 2.4s\u001b[0K\u001b[1G164.9 MiB [] 10% 2.3s\u001b[0K\u001b[1G164.9 MiB [] 11% 2.2s\u001b[0K\u001b[1G164.9 MiB [] 12% 2.1s\u001b[0K\u001b[1G164.9 MiB [] 13% 2.0s\u001b[0K\u001b[1G164.9 MiB [] 15% 1.9s\u001b[0K\u001b[1G164.9 MiB [] 16% 1.8s\u001b[0K\u001b[1G164.9 MiB [] 17% 1.7s\u001b[0K\u001b[1G164.9 MiB [] 18% 1.7s\u001b[0K\u001b[1G164.9 MiB [] 19% 1.6s\u001b[0K\u001b[1G164.9 MiB [] 20% 1.6s\u001b[0K\u001b[1G164.9 MiB [] 22% 1.5s\u001b[0K\u001b[1G164.9 MiB [] 23% 1.5s\u001b[0K\u001b[1G164.9 MiB [] 23% 1.6s\u001b[0K\u001b[1G164.9 MiB [] 24% 1.6s\u001b[0K\u001b[1G164.9 MiB [] 25% 1.5s\u001b[0K\u001b[1G164.9 MiB [] 27% 1.4s\u001b[0K\u001b[1G164.9 MiB [] 28% 1.4s\u001b[0K\u001b[1G164.9 MiB [] 30% 1.3s\u001b[0K\u001b[1G164.9 MiB [] 31% 1.3s\u001b[0K\u001b[1G164.9 MiB [] 33% 1.2s\u001b[0K\u001b[1G164.9 MiB [] 34% 1.2s\u001b[0K\u001b[1G164.9 MiB [] 35% 1.1s\u001b[0K\u001b[1G164.9 MiB [] 37% 1.1s\u001b[0K\u001b[1G164.9 MiB [] 38% 1.0s\u001b[0K\u001b[1G164.9 MiB [] 40% 1.0s\u001b[0K\u001b[1G164.9 MiB [] 41% 1.0s\u001b[0K\u001b[1G164.9 MiB [] 43% 0.9s\u001b[0K\u001b[1G164.9 MiB [] 44% 0.9s\u001b[0K\u001b[1G164.9 MiB [] 45% 0.9s\u001b[0K\u001b[1G164.9 MiB [] 46% 0.9s\u001b[0K\u001b[1G164.9 MiB [] 47% 0.9s\u001b[0K\u001b[1G164.9 MiB [] 48% 0.8s\u001b[0K\u001b[1G164.9 MiB [] 50% 0.8s\u001b[0K\u001b[1G164.9 MiB [] 51% 0.8s\u001b[0K\u001b[1G164.9 MiB [] 53% 0.7s\u001b[0K\u001b[1G164.9 MiB [] 54% 0.7s\u001b[0K\u001b[1G164.9 MiB [] 55% 0.7s\u001b[0K\u001b[1G164.9 MiB [] 57% 0.7s\u001b[0K\u001b[1G164.9 MiB [] 58% 0.6s\u001b[0K\u001b[1G164.9 MiB [] 59% 0.6s\u001b[0K\u001b[1G164.9 MiB [] 61% 0.6s\u001b[0K\u001b[1G164.9 MiB [] 62% 0.6s\u001b[0K\u001b[1G164.9 MiB [] 63% 0.5s\u001b[0K\u001b[1G164.9 MiB [] 64% 0.5s\u001b[0K\u001b[1G164.9 MiB [] 65% 0.5s\u001b[0K\u001b[1G164.9 MiB [] 66% 0.5s\u001b[0K\u001b[1G164.9 MiB [] 67% 0.5s\u001b[0K\u001b[1G164.9 MiB [] 68% 0.5s\u001b[0K\u001b[1G164.9 MiB [] 69% 0.5s\u001b[0K\u001b[1G164.9 MiB [] 70% 0.5s\u001b[0K\u001b[1G164.9 MiB [] 71% 0.4s\u001b[0K\u001b[1G164.9 MiB [] 72% 0.4s\u001b[0K\u001b[1G164.9 MiB [] 73% 0.4s\u001b[0K\u001b[1G164.9 MiB [] 74% 0.4s\u001b[0K\u001b[1G164.9 MiB [] 76% 0.4s\u001b[0K\u001b[1G164.9 MiB [] 77% 0.3s\u001b[0K\u001b[1G164.9 MiB [] 79% 0.3s\u001b[0K\u001b[1G164.9 MiB [] 80% 0.3s\u001b[0K\u001b[1G164.9 MiB [] 82% 0.3s\u001b[0K\u001b[1G164.9 MiB [] 83% 0.2s\u001b[0K\u001b[1G164.9 MiB [] 85% 0.2s\u001b[0K\u001b[1G164.9 MiB [] 87% 0.2s\u001b[0K\u001b[1G164.9 MiB [] 88% 0.2s\u001b[0K\u001b[1G164.9 MiB [] 89% 0.1s\u001b[0K\u001b[1G164.9 MiB [] 90% 0.1s\u001b[0K\u001b[1G164.9 MiB [] 91% 0.1s\u001b[0K\u001b[1G164.9 MiB [] 92% 0.1s\u001b[0K\u001b[1G164.9 MiB [] 93% 0.1s\u001b[0K\u001b[1G164.9 MiB [] 94% 0.1s\u001b[0K\u001b[1G164.9 MiB [] 95% 0.1s\u001b[0K\u001b[1G164.9 MiB [] 96% 0.0s\u001b[0K\u001b[1G164.9 MiB [] 97% 0.0s\u001b[0K\u001b[1G164.9 MiB [] 98% 0.0s\u001b[0K\u001b[1G164.9 MiB [] 99% 0.0s\u001b[0K\u001b[1G164.9 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 134.0.6998.35 (playwright build v1161) downloaded to /root/.cache/ms-playwright/chromium-1161\n",
            "Downloading FFMPEG playwright build v1011\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/ffmpeg/1011/ffmpeg-linux.zip\u001b[22m\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 5% 0.3s\u001b[0K\u001b[1G2.3 MiB [] 24% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 70% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1011 downloaded to /root/.cache/ms-playwright/ffmpeg-1011\n",
            "Downloading Chromium Headless Shell 134.0.6998.35 (playwright build v1161)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1161/chromium-headless-shell-linux.zip\u001b[22m\n",
            "\u001b[1G100.9 MiB [] 0% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 0% 11.5s\u001b[0K\u001b[1G100.9 MiB [] 0% 5.2s\u001b[0K\u001b[1G100.9 MiB [] 1% 2.9s\u001b[0K\u001b[1G100.9 MiB [] 3% 2.1s\u001b[0K\u001b[1G100.9 MiB [] 4% 1.8s\u001b[0K\u001b[1G100.9 MiB [] 5% 1.6s\u001b[0K\u001b[1G100.9 MiB [] 6% 1.6s\u001b[0K\u001b[1G100.9 MiB [] 8% 1.5s\u001b[0K\u001b[1G100.9 MiB [] 9% 1.4s\u001b[0K\u001b[1G100.9 MiB [] 10% 1.5s\u001b[0K\u001b[1G100.9 MiB [] 11% 1.4s\u001b[0K\u001b[1G100.9 MiB [] 13% 1.3s\u001b[0K\u001b[1G100.9 MiB [] 14% 1.3s\u001b[0K\u001b[1G100.9 MiB [] 16% 1.2s\u001b[0K\u001b[1G100.9 MiB [] 17% 1.2s\u001b[0K\u001b[1G100.9 MiB [] 18% 1.2s\u001b[0K\u001b[1G100.9 MiB [] 20% 1.1s\u001b[0K\u001b[1G100.9 MiB [] 22% 1.0s\u001b[0K\u001b[1G100.9 MiB [] 24% 1.0s\u001b[0K\u001b[1G100.9 MiB [] 26% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 27% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 29% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 31% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 33% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 35% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 37% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 38% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 40% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 42% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 44% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 46% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 49% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 50% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 53% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 54% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 55% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 56% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 58% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 59% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 60% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 62% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 63% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 64% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 65% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 67% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 68% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 69% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 70% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 71% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 72% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 73% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 75% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 76% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 78% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 79% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 81% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 82% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 83% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 84% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 85% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 86% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 88% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 90% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 91% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 93% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 95% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 96% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 98% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 99% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium Headless Shell 134.0.6998.35 (playwright build v1161) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1161\n",
            "\u001b[32m[COMPLETE] ● Playwright installation completed successfully.\u001b[0m\n",
            "\u001b[36m[INIT].... → Starting database initialization...\u001b[0m\n",
            "\u001b[36m[COMPLETE] ● Database backup created at: /root/.crawl4ai/crawl4ai.db.backup_20250321_112612\u001b[0m\n",
            "\u001b[36m[INIT].... → Starting database migration...\u001b[0m\n",
            "\u001b[32m[COMPLETE] ● Migration completed. 0 records processed.\u001b[0m\n",
            "\u001b[32m[COMPLETE] ● Database initialization completed successfully.\u001b[0m\n",
            "\u001b[32m[COMPLETE] ● Post-installation setup completed!\u001b[0m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! crawl4ai-doctor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juWlb4qSc3UA",
        "outputId": "e6cd1fc6-192a-48cd-fc2f-4643f9686442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m[INIT].... → Running Crawl4AI health check...\u001b[0m\n",
            "\u001b[36m[INIT].... → Crawl4AI 0.5.0.post4\u001b[0m\n",
            "\u001b[36m[TEST].... ℹ Testing crawling capabilities...\u001b[0m\n",
            "\u001b[36m[EXPORT].. ℹ Exporting PDF and taking screenshot took 1.85s\u001b[0m\n",
            "\u001b[32m[FETCH]... ↓ https://crawl4ai.com... | Status: \u001b[32mTrue\u001b[0m | Time: 3.72s\u001b[0m\n",
            "\u001b[36m[SCRAPE].. ◆ https://crawl4ai.com... | Time: 0.053s\u001b[0m\n",
            "\u001b[32m[COMPLETE] ● https://crawl4ai.com... | Status: \u001b[32mTrue\u001b[0m | Total: \u001b[33m3.78s\u001b[0m\u001b[0m\n",
            "\u001b[32m[COMPLETE] ● ✅ Crawling test passed!\u001b[0m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "import asyncio\n",
        "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n",
        "\n",
        "nest_asyncio.apply()  # 기존 루프에 중첩 실행 허용\n",
        "\n",
        "# 글로벌 변수 선언\n",
        "result = None\n",
        "\n",
        "async def main():\n",
        "    global result  # 전역 변수로 지정\n",
        "    async with AsyncWebCrawler() as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://www.example.com\",\n",
        "        )\n",
        "        print(result.markdown[:300])\n",
        "\n",
        "await main()\n",
        "\n",
        "# 이후 result를 자유롭게 사용 가능\n",
        "# 예: print(result.html) 또는 result.title 등\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Qir4YEMc7hJ",
        "outputId": "1623d975-9fd5-4b44-d1a6-208c2684eb07"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INIT].... → Crawl4AI 0.5.0.post4\n",
            "[FETCH]... ↓ https://www.example.com... | Status: True | Time: 1.68s\n",
            "[SCRAPE].. ◆ https://www.example.com... | Time: 0.003s\n",
            "[COMPLETE] ● https://www.example.com... | Status: True | Total: 1.71s\n",
            "# Example Domain\n",
            "This domain is for use in illustrative examples in documents. You may use this domain in literature without prior coordination or asking for permission.\n",
            "[More information...](https://www.iana.org/domains/example)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UmB-UvBiUXw",
        "outputId": "128969ef-a91c-425f-c848-27086f8089c9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CrawlResultContainer([CrawlResult(url='https://www.example.com', html='<!DOCTYPE html><html><head>\\n    <title>Example Domain</title>\\n\\n    <meta charset=\"utf-8\">\\n    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\">\\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\n    <style type=\"text/css\">\\n    body {\\n        background-color: #f0f0f2;\\n        margin: 0;\\n        padding: 0;\\n        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\\n        \\n    }\\n    div {\\n        width: 600px;\\n        margin: 5em auto;\\n        padding: 2em;\\n        background-color: #fdfdff;\\n        border-radius: 0.5em;\\n        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\\n    }\\n    a:link, a:visited {\\n        color: #38488f;\\n        text-decoration: none;\\n    }\\n    @media (max-width: 700px) {\\n        div {\\n            margin: 0 auto;\\n            width: auto;\\n        }\\n    }\\n    </style>    \\n</head>\\n\\n<body>\\n<div>\\n    <h1>Example Domain</h1>\\n    <p>This domain is for use in illustrative examples in documents. You may use this\\n    domain in literature without prior coordination or asking for permission.</p>\\n    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\\n</div>\\n\\n\\n</body></html>', success=True, cleaned_html='\\n<div>\\n<h1>Example Domain</h1>\\n<p>This domain is for use in illustrative examples in documents. You may use this\\n  domain in literature without prior coordination or asking for permission.</p>\\n<p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\\n</div>\\n', media={'images': [], 'videos': [], 'audios': []}, links={'internal': [], 'external': [{'href': 'https://www.iana.org/domains/example', 'text': 'More information...', 'title': '', 'base_domain': 'iana.org'}]}, downloaded_files=None, js_execution_result=None, screenshot=None, pdf=None, extracted_content=None, metadata={'title': 'Example Domain', 'description': None, 'keywords': None, 'author': None}, error_message='', session_id=None, response_headers={'accept-ranges': 'bytes', 'alt-svc': 'h3=\":443\"; ma=93600,h3-29=\":443\"; ma=93600,quic=\":443\"; ma=93600; v=\"43\"', 'cache-control': 'max-age=1098', 'content-encoding': 'gzip', 'content-length': '648', 'content-type': 'text/html', 'date': 'Fri, 21 Mar 2025 11:52:20 GMT', 'etag': '\"84238dfc8092e5d9c0dac8ef93371a07:1736799080.121134\"', 'last-modified': 'Mon, 13 Jan 2025 20:11:20 GMT', 'vary': 'Accept-Encoding'}, status_code=200, ssl_certificate=None, dispatch_result=None, redirected_url='https://www.example.com/')])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# body 태그 내 텍스트만 추출\n",
        "soup = BeautifulSoup(result.html, \"html.parser\")\n",
        "body_text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
        "\n",
        "print(body_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGxGsZhPqy55",
        "outputId": "467926d4-6b8d-4cc4-99e3-82f3d3870c95"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example Domain\n",
            "This domain is for use in illustrative examples in documents. You may use this\n",
            "    domain in literature without prior coordination or asking for permission.\n",
            "More information...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(soup)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2YxI3rmraK6",
        "outputId": "72ed48c5-6c3f-4694-b32d-aaf4845c7b1f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!DOCTYPE html>\n",
            "<html><head>\n",
            "<title>Example Domain</title>\n",
            "<meta charset=\"utf-8\"/>\n",
            "<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-type\"/>\n",
            "<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
            "<style type=\"text/css\">\n",
            "    body {\n",
            "        background-color: #f0f0f2;\n",
            "        margin: 0;\n",
            "        padding: 0;\n",
            "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
            "        \n",
            "    }\n",
            "    div {\n",
            "        width: 600px;\n",
            "        margin: 5em auto;\n",
            "        padding: 2em;\n",
            "        background-color: #fdfdff;\n",
            "        border-radius: 0.5em;\n",
            "        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n",
            "    }\n",
            "    a:link, a:visited {\n",
            "        color: #38488f;\n",
            "        text-decoration: none;\n",
            "    }\n",
            "    @media (max-width: 700px) {\n",
            "        div {\n",
            "            margin: 0 auto;\n",
            "            width: auto;\n",
            "        }\n",
            "    }\n",
            "    </style>\n",
            "</head>\n",
            "<body>\n",
            "<div>\n",
            "<h1>Example Domain</h1>\n",
            "<p>This domain is for use in illustrative examples in documents. You may use this\n",
            "    domain in literature without prior coordination or asking for permission.</p>\n",
            "<p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n",
            "</div>\n",
            "</body></html>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rendered HTML의 내용을 직접 추출"
      ],
      "metadata": {
        "id": "5a8DJkhvrtXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from playwright.async_api import async_playwright\n",
        "# JS도 Rn=endered HTML에서 직접 추출\n",
        "\n",
        "url = \"https://comic.naver.com/webtoon/weekday\"\n",
        "# url = \"https://quotes.toscrape.com/\"\n",
        "# url = 'https://example.com'\n",
        "\n",
        "async def test_browser():\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        page = await browser.new_page()\n",
        "        await page.goto(url)\n",
        "        print(f'Title: {await page.title()}')\n",
        "        await browser.close()\n",
        "\n",
        "asyncio.run(test_browser())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcPlGI43ri0V",
        "outputId": "c9eeae9b-4ee8-4262-9d6e-ad1197d77388"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: 요일전체 : 네이버 웹툰\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **[quickstart](https://docs.crawl4ai.com/core/quickstart/)**"
      ],
      "metadata": {
        "id": "NgiS6cV9sFpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "\n",
        "async def main():\n",
        "    async with AsyncWebCrawler() as crawler:\n",
        "        result = await crawler.arun(\"https://example.com\")\n",
        "        print(result.markdown[:300])  # Print first 300 chars\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dwh_jtq-r0uo",
        "outputId": "18c67702-41af-4588-ffb9-8b75d8c63135"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INIT].... → Crawl4AI 0.5.0.post4\n",
            "[FETCH]... ↓ https://example.com... | Status: True | Time: 1.91s\n",
            "[SCRAPE].. ◆ https://example.com... | Time: 0.003s\n",
            "[COMPLETE] ● https://example.com... | Status: True | Total: 1.94s\n",
            "# Example Domain\n",
            "This domain is for use in illustrative examples in documents. You may use this domain in literature without prior coordination or asking for permission.\n",
            "[More information...](https://www.iana.org/domains/example)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n",
        "\n",
        "async def main():\n",
        "    browser_conf = BrowserConfig(headless=True)  # or False to see the browser\n",
        "    run_conf = CrawlerRunConfig(\n",
        "        cache_mode=CacheMode.BYPASS\n",
        "    )\n",
        "\n",
        "    async with AsyncWebCrawler(config=browser_conf) as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://example.com\",\n",
        "            config=run_conf\n",
        "        )\n",
        "        print(result.markdown)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kv3xqw4gsV7E",
        "outputId": "7b86e322-781c-485b-8df1-5cdc159c2959"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INIT].... → Crawl4AI 0.5.0.post4\n",
            "[FETCH]... ↓ https://example.com... | Status: True | Time: 1.78s\n",
            "[SCRAPE].. ◆ https://example.com... | Time: 0.003s\n",
            "[COMPLETE] ● https://example.com... | Status: True | Total: 1.83s\n",
            "# Example Domain\n",
            "This domain is for use in illustrative examples in documents. You may use this domain in literature without prior coordination or asking for permission.\n",
            "[More information...](https://www.iana.org/domains/example)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n",
        "from crawl4ai.content_filter_strategy import PruningContentFilter\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "\n",
        "md_generator = DefaultMarkdownGenerator(\n",
        "    content_filter=PruningContentFilter(threshold=0.4, threshold_type=\"fixed\")\n",
        ")\n",
        "\n",
        "config = CrawlerRunConfig(\n",
        "    cache_mode=CacheMode.BYPASS,\n",
        "    markdown_generator=md_generator\n",
        ")\n",
        "\n",
        "async with AsyncWebCrawler() as crawler:\n",
        "    result = await crawler.arun(\"https://news.ycombinator.com\", config=config)\n",
        "    print(\"Raw Markdown length:\", len(result.markdown.raw_markdown))\n",
        "    print(\"Fit Markdown length:\", len(result.markdown.fit_markdown))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsT0MnVHsrM8",
        "outputId": "8328476a-1c4a-419c-f8e6-72f9e51c00d6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INIT].... → Crawl4AI 0.5.0.post4\n",
            "[FETCH]... ↓ https://news.ycombinator.com... | Status: True | Time: 1.39s\n",
            "[SCRAPE].. ◆ https://news.ycombinator.com... | Time: 0.231s\n",
            "[COMPLETE] ● https://news.ycombinator.com... | Status: True | Total: 1.65s\n",
            "Raw Markdown length: 16895\n",
            "Fit Markdown length: 14209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "openai_token = userdata.get('openapi')"
      ],
      "metadata": {
        "id": "9F8IzDpNtlrg"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n",
        "from crawl4ai import LLMConfig\n",
        "\n",
        "# Generate a schema (one-time cost)\n",
        "html = \"<div class='product'><h2>Gaming Laptop</h2><span class='price'>$999.99</span></div>\"\n",
        "\n",
        "\n",
        "# Using OpenAI (requires API token)\n",
        "schema = JsonCssExtractionStrategy.generate_schema(\n",
        "    html,\n",
        "    llm_config = LLMConfig(provider=\"openai/gpt-4o\",api_token= openai_token)  # Required for OpenAI\n",
        ")\n",
        "\n",
        "# Use the schema for fast, repeated extractions\n",
        "strategy = JsonCssExtractionStrategy(schema)"
      ],
      "metadata": {
        "id": "Ndcqqn2RswRs"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strategy.schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX2Sj-Bst0co",
        "outputId": "80346134-67b8-4c93-da31-46fda1732eb9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'Product Listing',\n",
              " 'baseSelector': '.product',\n",
              " 'fields': [{'name': 'product_name', 'selector': 'h2', 'type': 'text'},\n",
              "  {'name': 'price', 'selector': '.price', 'type': 'text'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://comic.naver.com/webtoon/weekday\"\n",
        "res = requests.get(url)\n",
        "\n",
        "soup = BeautifulSoup(res.text, \"lxml\")  # HTML 파싱"
      ],
      "metadata": {
        "id": "Td_2tNuVvpAA"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using OpenAI (requires API token)\n",
        "schema = JsonCssExtractionStrategy.generate_schema(\n",
        "    soup,\n",
        "    llm_config = LLMConfig(provider=\"openai/gpt-4o\",api_token= openai_token)  # Required for OpenAI\n",
        ")\n",
        "\n",
        "# Use the schema for fast, repeated extractions\n",
        "strategy = JsonCssExtractionStrategy(schema)"
      ],
      "metadata": {
        "id": "vLF2UgOuvdu9"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strategy.schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZ2ewhIuv7tu",
        "outputId": "7c67e4e3-8a8c-44d6-f467-dde36930ff0f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'Naver Webtoon Page',\n",
              " 'baseSelector': 'head',\n",
              " 'fields': [{'name': 'title', 'selector': 'title', 'type': 'text'},\n",
              "  {'name': 'favicon',\n",
              "   'selector': \"link[rel='icon']\",\n",
              "   'type': 'attribute',\n",
              "   'attribute': 'href'},\n",
              "  {'name': 'canonical_url',\n",
              "   'selector': \"link[rel='canonical']\",\n",
              "   'type': 'attribute',\n",
              "   'attribute': 'href'},\n",
              "  {'name': 'google_site_verification',\n",
              "   'selector': \"meta[name='google-site-verification']\",\n",
              "   'type': 'attribute',\n",
              "   'attribute': 'content'},\n",
              "  {'name': 'charset',\n",
              "   'selector': 'meta[charset]',\n",
              "   'type': 'attribute',\n",
              "   'attribute': 'charset'},\n",
              "  {'name': 'og_type',\n",
              "   'selector': \"meta[property='og:type']\",\n",
              "   'type': 'attribute',\n",
              "   'attribute': 'content'},\n",
              "  {'name': 'og_author',\n",
              "   'selector': \"meta[property='og:article:author']\",\n",
              "   'type': 'attribute',\n",
              "   'attribute': 'content'},\n",
              "  {'name': 'og_author_url',\n",
              "   'selector': \"meta[property='og:article:author:url']\",\n",
              "   'type': 'attribute',\n",
              "   'attribute': 'content'},\n",
              "  {'name': 'og_title',\n",
              "   'selector': \"meta[property='og:title']\",\n",
              "   'type': 'attribute',\n",
              "   'attribute': 'content'},\n",
              "  {'name': 'og_image',\n",
              "   'selector': \"meta[property='og:image']\",\n",
              "   'type': 'attribute',\n",
              "   'attribute': 'content'},\n",
              "  {'name': 'og_description',\n",
              "   'selector': \"meta[property='og:description']\",\n",
              "   'type': 'attribute',\n",
              "   'attribute': 'content'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: strategy 에서 text를 추출하려면\n",
        "\n",
        "# ... (Your existing code)\n",
        "\n",
        "# Assuming 'strategy' is your JsonCssExtractionStrategy instance\n",
        "if strategy and strategy.schema:\n",
        "  print(\"Extracted text from schema:\")\n",
        "  for key, value in strategy.schema.items():\n",
        "    print(f\"Key: {key}, Value: {value['css_selector']}\")\n",
        "    # You might need to adapt this based on how your schema is structured\n",
        "\n",
        "    # Example: Extract the text content of elements matching the CSS selector\n",
        "    elements = soup.select(value['css_selector'])\n",
        "    for element in elements:\n",
        "      print(element.get_text(strip=True))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "dmI7Q6AVwAMt",
        "outputId": "5ce26714-c786-40fd-cea6-c6bb556563f8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted text from schema:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "string indices must be integers, not 'str'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-7a7ec0a6c3e8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extracted text from schema:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key: {key}, Value: {value['css_selector']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# You might need to adapt this based on how your schema is structured\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'strategy' is your JsonCssExtractionStrategy instance\n",
        "if strategy and strategy.schema:\n",
        "  print(\"Extracted data from schema:\")\n",
        "  for key, value in strategy.schema.items():\n",
        "    print(f\"Key: {key}, Value: {value}\")  # Print the extracted value directly\n",
        "    # If you need to access specific parts of the extracted data,\n",
        "    # you'll need to inspect its structure and adjust the code accordingly.\n",
        "    # For example, if 'value' is a dictionary, you might access specific fields like this:\n",
        "    # if isinstance(value, dict) and 'text' in value:\n",
        "    #   print(f\"  Text: {value['text']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOLsate8wKQ3",
        "outputId": "c91aa7ed-04dc-4c41-a295-721627b4deac"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted data from schema:\n",
            "Key: name, Value: Naver Webtoon Page\n",
            "Key: baseSelector, Value: head\n",
            "Key: fields, Value: [{'name': 'title', 'selector': 'title', 'type': 'text'}, {'name': 'favicon', 'selector': \"link[rel='icon']\", 'type': 'attribute', 'attribute': 'href'}, {'name': 'canonical_url', 'selector': \"link[rel='canonical']\", 'type': 'attribute', 'attribute': 'href'}, {'name': 'google_site_verification', 'selector': \"meta[name='google-site-verification']\", 'type': 'attribute', 'attribute': 'content'}, {'name': 'charset', 'selector': 'meta[charset]', 'type': 'attribute', 'attribute': 'charset'}, {'name': 'og_type', 'selector': \"meta[property='og:type']\", 'type': 'attribute', 'attribute': 'content'}, {'name': 'og_author', 'selector': \"meta[property='og:article:author']\", 'type': 'attribute', 'attribute': 'content'}, {'name': 'og_author_url', 'selector': \"meta[property='og:article:author:url']\", 'type': 'attribute', 'attribute': 'content'}, {'name': 'og_title', 'selector': \"meta[property='og:title']\", 'type': 'attribute', 'attribute': 'content'}, {'name': 'og_image', 'selector': \"meta[property='og:image']\", 'type': 'attribute', 'attribute': 'content'}, {'name': 'og_description', 'selector': \"meta[property='og:description']\", 'type': 'attribute', 'attribute': 'content'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strategy.schema['name']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iVzGAuI9wSjP",
        "outputId": "57b188e1-112b-4e1d-8905-58218f2b632b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Naver Webtoon Page'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strategy.schema['baseSelector']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "h-pzxh5iwZbf",
        "outputId": "76d313d9-b65d-4e36-df85-2a73f2f670f0"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'head'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gK6XeKYSwf_h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}